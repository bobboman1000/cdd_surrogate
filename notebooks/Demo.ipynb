{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-08T12:29:05.148217Z",
     "end_time": "2023-05-08T12:29:08.481060Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "import torch\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from fccd.data.modules import CDDDataModule\n",
    "from fccd.data.datasets import TabularDataset\n",
    "from fccd.models.LSTM import RepeatedInputLSTM, EncodedHiddenStateLSTM, ChunkingEncodedHiddenStateLSTM\n",
    "from fccd.models.baseline import KNNBaseline\n",
    "from fccd.models.GBT import LightGBMModel\n",
    "from fccd.util import plot_prediction_vs_truth, plot_prediction_vs_truth_sklearn, limit_psd\n",
    "from fccd.util import collect_dataloader\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and prepare data\n",
    "\n",
    "First load and prepare the data. All other notebooks assume the data to be located in \"$PROJ_ROOT/data/processed/\".\n",
    "\n",
    "We first load the data, then we apply the limit_psd step. If the PSD estiamte is higher than the actual highest stress level, this may cause errors. Hence we limit the PSDs to be lesser or equal to the highest stress level of the corresponding series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "cu_125 = pd.read_csv(\"../data/processed/cu_125.csv\", index_col=0)\n",
    "cu_1000 = pd.read_csv(\"../data/processed/cu_1000.csv\", index_col=0)\n",
    "\n",
    "al_125 = pd.read_csv(\"../data/processed/al_125.csv\", index_col=0)\n",
    "al_1000 = pd.read_csv(\"../data/processed/al_1000.csv\", index_col=0)\n",
    "\n",
    "au_125 = pd.read_csv(\"../data/processed/au_125.csv\", index_col=0)\n",
    "au_1000 = pd.read_csv(\"../data/processed/au_1000.csv\", index_col=0)\n",
    "\n",
    "data = pd.concat([cu_125])\n",
    "#data_single = pd.concat([cu_125, cu_1000, al_125, al_1000, au_125, au_1000])   # Loading all data (Memory-heavy)\n",
    "\n",
    "data = data.reset_index(drop=True)\n",
    "\n",
    "# GPa to MPa, optional\n",
    "data[\"stress\"] = data[\"stress\"] * 1000\n",
    "data[\"psd\"] = data[\"psd\"] * 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Data\n",
    "data = limit_psd(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Dataset\n",
    "\n",
    "For creating a dataset we create LightningDatamodules called CDDDataModule. The modules and its underlying datasets handle the scaling and preprocessing\n",
    "\n",
    "For Mulit-material we can just use multiple datasets, for multi-target we need to specify the multi-target parameter."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Single target datasets\n",
    "\n",
    "data:           the dataframe\n",
    "target:         name of the target in df\n",
    "psd:            name of psd in df\n",
    "t:              name of time index in df\n",
    "group:          name of series identifier in df\n",
    "\n",
    "drop_cols:      get dropped during preprocessing, exlcude obselete columns and different targets\n",
    "\n",
    "transform:      provide a sklearn scaler if scaling is desired\n",
    "\n",
    "split_dataset:  true if computation of split is needed, (Use true if using the LSTM+PDP model, False otherwise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a single-target dataset\n",
    "\n",
    "stress_dm = CDDDataModule(\n",
    "    data,\n",
    "    target=\"dislocation\",\n",
    "    psd=\"psd\",\n",
    "    group=\"id\",\n",
    "    drop_cols=[\"strain\", \"time_ns\", \"stress\"],\n",
    "    time=\"t\",\n",
    "    batch_size=64,\n",
    "    categoricals=[\"material\", \"euler_angles\", \"mesh\"],\n",
    "    num_dataloader_workers=8,\n",
    "    transform=MinMaxScaler,\n",
    "    split_dataset=True,\n",
    ")\n",
    "stress_dm.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a multi-target dataset\n",
    "# The target does not matter actually\n",
    "\n",
    "stress_dm = CDDDataModule(\n",
    "    data,\n",
    "    psd=\"psd\",\n",
    "    target=\"all\",\n",
    "    multi_target=True,\n",
    "    group=\"id\",\n",
    "    drop_cols=[\"time_ns\"],\n",
    "    time=\"t\",\n",
    "    batch_size=64,\n",
    "    categoricals=[\"material\", \"euler_angles\", \"mesh\"],\n",
    "    num_dataloader_workers=0,\n",
    "    transform=MinMaxScaler,\n",
    "    split_dataset=True,\n",
    ")\n",
    "stress_dm.setup()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the models\n",
    "\n",
    "Training the models stays the same for all three single/multi-material and single/multi-target, we just need to provide the proper datamodule.\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sklearn models\n",
    "\n",
    "Training models following the sklaern API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sklearn_model(model, datamodule):\n",
    "    train_dataloader = datamodule.train_dataloader()\n",
    "    val_dataloader = datamodule.train_dataloader()\n",
    "\n",
    "    X_train, y_train = collect_dataloader(train_dataloader)\n",
    "    X_val, y_val = collect_dataloader(val_dataloader)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    test_dataloader = datamodule.test_dataloader()\n",
    "    X_test, y_test = collect_dataloader(test_dataloader)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    test_loss = mean_squared_error(y_test, y_test_pred)\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict([[0.01, 0.2, 0.9]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNNBaseline(1)\n",
    "train_sklearn_model(knn, datamodule=stress_dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm = LightGBMModel()\n",
    "train_sklearn_model(gbm, stress_dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prediction_vs_truth_sklearn(stress_dm.test_dataloader(), {\"gbm\": gbm, \"knn_bs\": knn}, 2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeated Input LSTM\n",
    "\n",
    "This LSTM takes all parameters at every time step as input plus past predictions resulting in a structure like\n",
    "with $x_i$ and i the ith\n",
    "\n",
    "($x_{1}$ $x_{2}$ $x_{3}$ $0$) -> ($x_{1}$ $x_{2}$ $x_{3}$ $y_0$) -> ... -> ($x_{1}$ $x_{2}$ $x_{3}$ $y_{t-1}$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repin = RepeatedInputLSTM(n_hidden=30, input_size=20)\n",
    "\n",
    "# For multi-target just specify the corresponding output size\n",
    "repin = RepeatedInputLSTM(n_hidden=30, input_size=20, output_size=3)\n",
    "\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=3e-10, patience=10, verbose=False, mode=\"min\")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=10,                      # Max  training epochs: Hard limit\n",
    "    accelerator=\"gpu\",                  # cpu/gpu/mps Use cuda gpu for fast training\n",
    "    callbacks=[early_stop_callback],    # Early stopping if accuracy does not increase on validation set (confifgure delta and patience above)\n",
    "    auto_lr_find=True                   # Automatically find learning rate for LSTM, called in trainer.tune()\n",
    ")\n",
    "trainer.tune(repin, datamodule=stress_dm)\n",
    "trainer.fit(repin, datamodule=stress_dm)\n",
    "#trainer.test(repin, datamodule=stress_dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions of model vs ground truth\n",
    "plot_prediction_vs_truth(stress_dm.test_dataloader(), {\"repin\": repin}, 2, 400)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoded Hidden State LSTM\n",
    "\n",
    "LSTM with initial sequence input 0 that uses encoding of input parameters as initial hidden and cell state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_hidden_lstm = EncodedHiddenStateLSTM(input_size=20, lstm_layers=1, lstm_hidden_size=30, mlp_hidden_layers=2, output_size=1)\n",
    "\n",
    "# For multi target just specify to proper output size\n",
    "encoded_hidden_lstm_multi_target = EncodedHiddenStateLSTM(input_size=20, lstm_layers=1, lstm_hidden_size=30, mlp_hidden_layers=2, output_size=3)\n",
    "\n",
    "\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=3e-6, patience=100, verbose=False, mode=\"min\")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=1500, \n",
    "    accelerator=\"gpu\",\n",
    "    callbacks=[early_stop_callback],\n",
    "    auto_lr_find=True\n",
    ")\n",
    "trainer.tune(encoded_hidden_lstm, datamodule=stress_dm)\n",
    "trainer.fit(encoded_hidden_lstm, datamodule=stress_dm)\n",
    "#trainer.test(encoded_decoded, datamodule=stress_dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prediction_vs_truth(stress_dm.train_dataloader(), {\"encoded_decoded\": encoded_hidden_lstm}, 2, 400)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training LSTM+PDP\n",
    "\n",
    "Training the LSTM+PDP Hybrid. First we need to create a dataset to train the PDP-index estimate, then we create a model and pass a wrapper method that provides estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = stress_dm.train_dataset()\n",
    "val_data = stress_dm.val_dataset()\n",
    "test_data = stress_dm.test_dataset()\n",
    "\n",
    "# Create dataset to train PDP index estimator\n",
    "psd_train_data = TabularDataset.psd_from_cdd_dataset(train_data)\n",
    "psd_val_data = TabularDataset.psd_from_cdd_dataset(val_data)\n",
    "psd_test_data = TabularDataset.psd_from_cdd_dataset(test_data)\n",
    "\n",
    "x_train, y_train = psd_train_data.dataset\n",
    "x_val, y_val = psd_val_data.dataset\n",
    "x_test, y_test = psd_test_data.dataset\n",
    "\n",
    "stackedx_train = torch.cat([x_train, x_val], dim=0)\n",
    "stackedy_train = torch.cat([y_train, y_val], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and fit estimator\n",
    "rf = RandomForestRegressor(n_estimators=1000)\n",
    "rf.fit(stackedx_train, stackedy_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper method to provide PDP index estimates\n",
    "def estimate_rounded_rf(static_parameters, psd_data, device):\n",
    "    regression_input = torch.hstack([static_parameters, psd_data])\n",
    "    regression_input = regression_input.detach().clone()\n",
    "    \n",
    "    if regression_input.device != \"cpu\":\n",
    "        regression_input = regression_input.cpu()\n",
    "\n",
    "    prediction = rf.predict(regression_input)\n",
    "    result = torch.from_numpy(prediction)\n",
    "    result = result.to(device)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chenc = ChunkingEncodedHiddenStateLSTM(20, 30, 5, 1).with_estimator(estimate_rounded_rf)\n",
    "\n",
    "# For multi-target specify the outuput size and the index of the target that corresponds to stress. The model predicts stress using the PDP estimator + the LSTM\n",
    "chenc_multi = ChunkingEncodedHiddenStateLSTM(20, 30, 5, 1, output_size=3, target_stress_idx=0).with_estimator(estimate_rounded_rf)\n",
    "\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=3e-10, patience=100, verbose=False, mode=\"min\")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=1500,\n",
    "    accelerator=\"gpu\",\n",
    "    callbacks=[early_stop_callback],\n",
    "    auto_lr_find=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer.tune(chenc, datamodule=stress_dm)\n",
    "trainer.fit(chenc, datamodule=stress_dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prediction_vs_truth(stress_dm.train_dataloader(), {\"chenc\": chenc}, 5, 400)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b9b62b7a9ac584c2fba11230ff8573e43de2f5694f1d7b472c0f899f2dd6b994"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
